{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AD for periodic time series\n",
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, glob\n",
    "from datetime import timedelta, datetime\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "import scipy.stats\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the dataset\n",
    "dataset_name = \"badec\"\n",
    "\n",
    "# Duration of the training\n",
    "train_length = \"2w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path with the .csv train set files and path for the .csv validation file\n",
    "csv_filenames = \"/notebooks/EIA_2022/data/\" + dataset_name + \"/train/clean_data/\" + train_length + \"/*.csv\"\n",
    "val_filename = \"/notebooks/EIA_2022/data/\" + dataset_name + \"/val/val_\" + dataset_name + \".csv\"\n",
    "test_filename = \"/notebooks/EIA_2022/data/\" + dataset_name + \"/test/test_\" + dataset_name + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = read_csv(val_filename)\n",
    "test_set = read_csv(test_filename)\n",
    "\n",
    "anomalies = read_csv(\"/notebooks/EIA_2022/data/anomalies_\" + dataset_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train set files need to be merged. Taking an integer number of cycles is necessary to have a periodic\n",
    "# or quasi-periodic anomaly free train set time series.\n",
    "\n",
    "def keep_cycles(df, threshold = 50): # put the train set together to have an integer number of cycles\n",
    "    '''\n",
    "    Merge the train set files by taking an integer number of cycles.\n",
    "    A cycle starts when there is a variation greater than threshold with respect to the previous point.\n",
    "    '''\n",
    "    df[\"variation\"] = df[\"device_consumption\"].diff()\n",
    "    mask = df[\"variation\"] > threshold # see where there is the beginning of a new cycle\n",
    "    beginnings = df[mask].index.values\n",
    "    if len(beginnings) > 0:\n",
    "        first_beginning = beginnings[0] # index at which the first cycle begins\n",
    "        last_beginning = beginnings[-1] - 1 # index at which the last cycle ends\n",
    "        \n",
    "        if first_beginning < last_beginning:\n",
    "            return df[first_beginning:last_beginning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "DLdbOk6OboUM",
    "outputId": "38d30cc5-ef2d-44c3-fe78-69b0eb2998d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = [None]*len(glob.glob(csv_filenames)) # Create an array of dataframes as long as the number of files\n",
    "\n",
    "# For each train set file, take an integer number of cycles and use the result as an element of the array\n",
    "for filename in glob.glob(csv_filenames):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f:    \n",
    "        file_number = int(filename.split(\"clean_\" + dataset_name + \"_\")[1].split(\".csv\")[0])\n",
    "        df = read_csv(f)\n",
    "        df = keep_cycles(df)\n",
    "        dfs[file_number] = df\n",
    "        \n",
    "# Build the train set by concatenating the parts of the smaller train sets\n",
    "train_set = pd.concat(dfs)  \n",
    "\n",
    "# Define a arbitrary starting datetime\n",
    "start_datetime = datetime(2020,1,1,0,0)\n",
    "\n",
    "# Define an arbitrary end datetime by adding the train set minutes to the start datetime\n",
    "# Assumption: the sampling time in the dataset is 1 minute\n",
    "end_datetime = start_datetime + timedelta(minutes = len(train_set)-1)\n",
    "\n",
    "# Define a date_range with the given dates and times\n",
    "timerange = pd.date_range(start=start_datetime, end=end_datetime, freq=\"1min\")\n",
    "\n",
    "# Use the defined time range as a column in Pandas\n",
    "train_set[\"ctime\"] = timerange\n",
    "\n",
    "# Reindex the dataframe\n",
    "train_set.reset_index(inplace=True)\n",
    "\n",
    "# Visualize the first 100 rows (samples) of the time series\n",
    "train_set.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_time_seconds(timestamps, timestamp_format):\n",
    "    '''\n",
    "    timestamps is an array of timestamps\n",
    "    timestamp_format is the format of the timestamp\n",
    "    '''\n",
    "    \n",
    "    # Assumption: the timestamps are equally spaced (as defined above)\n",
    "    # Compute the duration of an interval\n",
    "    time_0 = timestamps[0]\n",
    "    time_1 = timestamps[1]\n",
    "    time_interval = time_1 - time_0\n",
    "    \n",
    "    # Convert the interval duration in seconds\n",
    "    time_interval_seconds = pd.Timedelta(time_interval).total_seconds()\n",
    "    \n",
    "    return time_interval_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(xf, yf):\n",
    "    # Save the plot of the FFT of the test set\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    plt.plot(xf, np.abs(yf)) # plot only the absolute value of yf\n",
    "    \n",
    "    # Use a logarithmic scale\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    # Set plot description\n",
    "    plt.title('Power spectrum')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Power')\n",
    "    \n",
    "    # Save as .pdf to export without image data loss\n",
    "    plt.savefig(\"fft.pdf\", dpi=80)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_period(xf, yf):\n",
    "    '''\n",
    "    Given the Fourier transform power spectrum, find the highest peak\n",
    "    '''\n",
    "    max_idx = np.argmax(np.abs(yf)) # get the maximum power in the FFT\n",
    "    p = 1/xf[max_idx] # get the period (1/frequency) associated with the maximum power\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(time_series, timestamps, timestamp_format, first_N_timestamps, period_samples):    \n",
    "    '''\n",
    "    time_series is an array containing the time series values\n",
    "    timestamps is an array containing the timestamp values\n",
    "    timestamp_format is the format of the datetime\n",
    "    first_N_timestamps is the number of timestamps assumed to be part of an anomaly-free time series\n",
    "    period_samples is the number of samples in each period\n",
    "    '''\n",
    "    timestamp_seconds = get_sampling_time_seconds(timestamps.values, timestamp_format)\n",
    "    \n",
    "    # Save the plot of the first cycles\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), dpi=80)\n",
    "    ax.plot(np.arange(int(first_N_timestamps))*timestamp_seconds, time_series[:int(first_N_timestamps)])\n",
    "\n",
    "    # Draw a vertical line for each period\n",
    "    for ps in np.arange(period_samples*timestamp_seconds, first_N_timestamps*timestamp_seconds, period_samples*timestamp_seconds):\n",
    "        ax.vlines(ps, -50, 300, color='r')\n",
    "\n",
    "    ax.set_title('Power variation in a sequence of cycles')\n",
    "    ax.set_xlabel('Seconds')\n",
    "    ax.set_label('Power')\n",
    "    \n",
    "    plt.savefig(\"timeseries.pdf\", dpi=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft_periodicity(time_series, timestamps, timestamp_format):\n",
    "    ''' \n",
    "    time_series contains the values of the time series and is a pandas Series\n",
    "    timestamps contains the values of the time stamp and is a pandas Series\n",
    "    timestamp_format is the format of the datetime\n",
    "    '''\n",
    "    \n",
    "    timestamps = timestamps.values # recover the values of timestamps\n",
    "    time_series = time_series - time_series.mean() # subtract the mean from the time series values (used to perform FFT)\n",
    "\n",
    "    # Compute the FFT of the test set\n",
    "    yf = fft(time_series.values)\n",
    "    xf = fftfreq(len(time_series), 1)\n",
    "\n",
    "    # The Fourier transform is assumed to be symmetric due to the nature of the signal\n",
    "    mask = [idx for idx, val in enumerate(xf) if val >= 0] # create a mask to take only the positive frequencies\n",
    "    yf = yf[mask] # consider only the positive frequencies values\n",
    "\n",
    "    time_interval_seconds = get_sampling_time_seconds(timestamps, timestamp_format)\n",
    "    xf = xf[mask]/time_interval_seconds # convert to Hz (1/sample -> Hz)\n",
    "\n",
    "    # Estimate the main period from the highest peak in the Fourier transform\n",
    "    p = get_main_period(xf, yf)\n",
    "    p_samples = p/time_interval_seconds # period expressed in number of samples\n",
    "\n",
    "    plot_fft(xf, yf) # plots and saves the FFT\n",
    "\n",
    "    # Print information about the minutes and the samples of each period\n",
    "    # here, they must be the same for the presented data sets\n",
    "    print(\"The period is %.4f minutes, or %.4f samples\" %(p/60, p_samples))\n",
    "\n",
    "    return p_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "hsKR1_KmTeni",
    "outputId": "4f1f6b4c-0803-448e-c5dc-33c351e04f4d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "period = get_fft_periodicity(train_set[\"device_consumption\"], train_set[\"ctime\"], \"%Y-%m-%D %H:%M:%s\")\n",
    "period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Rstd(window1, window2):\n",
    "    # receives two uniformly sampled arrays\n",
    "    # returns the value of R_std\n",
    "    # Note: the value of R_std can be infinite, for example when a window has all the values at zero\n",
    "    std_current = np.std(window2)\n",
    "    std_previous = np.std(window1)\n",
    "    \n",
    "    # print(\"stdc\", std_current) # current std\n",
    "    # print(\"stdp\", std_previous) # previous std\n",
    "\n",
    "    return np.abs(std_current - std_previous)/std_previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_rstd(timeseries, period):\n",
    "    '''\n",
    "    timeseries is an array containing the values of the time series\n",
    "    period is the period associated with the time series\n",
    "    '''\n",
    "    \n",
    "    max_Rstd = 0\n",
    "    \n",
    "    # receives a uniformly sampled array and obtains the number of periods in the time series\n",
    "    number_of_periods = int(np.ceil(len(timeseries)/period))\n",
    "    \n",
    "    # For each couple of subsequent windows, compute the R_std\n",
    "    # Obtain the maximum R_std value in the considered part of the time series\n",
    "    for idx in range(number_of_periods-1):\n",
    "        array_idx = int(idx*period)\n",
    "        next_array_idx = int((idx+1)*period)\n",
    "        end_array_idx = next_array_idx + next_array_idx - array_idx\n",
    "        \n",
    "        window1 = timeseries[array_idx:next_array_idx]\n",
    "        window2 = timeseries[next_array_idx:end_array_idx]\n",
    "        \n",
    "        if len(window1) == len(window2):\n",
    "            Rstd = get_Rstd(window1, window2)\n",
    "            max_Rstd = max(Rstd, max_Rstd)\n",
    "        \n",
    "    return max_Rstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rstd_train = get_max_rstd(train_set[\"device_consumption\"].values, period)\n",
    "max_rstd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for grid search\n",
    "rstds = np.array(np.arange(1, 10, 0.1))*max_rstd_train\n",
    "f1s = [None]*len(rstds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation times and consumptions\n",
    "val_ctimes = np.array([datetime.strptime(a, '%Y-%m-%d %H:%M:%S') for a in val_set[\"ctime\"].values])\n",
    "val_consumptions = val_set[\"device_consumption\"].values\n",
    "\n",
    "# Find the GT anomaly intervals\n",
    "anomalies_starts = np.array([datetime.strptime(a, '%Y-%m-%d %H:%M:%S') for a in anomalies[\"start_date\"].values])\n",
    "anomalies_ends = np.array([datetime.strptime(a, '%Y-%m-%d %H:%M:%S') for a in anomalies[\"end_date\"].values])\n",
    "\n",
    "starts = [(ast - val_ctimes[0]).total_seconds()/60 for ast in anomalies_starts]\n",
    "ends = [(ast - val_ctimes[0]).total_seconds()/60 for ast in anomalies_ends]\n",
    "\n",
    "anomalies_time_intervals = np.array([[starts[i], ends[i]] for i, _ in enumerate(starts) if ends[i] > 0 and starts[i] < len(val_ctimes)])\n",
    "anomalies_time_intervals[anomalies_time_intervals < 0] = 0\n",
    "anomalies_time_intervals[anomalies_time_intervals > len(val_ctimes)] = len(val_ctimes)\n",
    "\n",
    "anomalies_time_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomalies(df_ts, period, ratio_threshold, pearson_threshold = 0.2):\n",
    "    ''' amount_normal_samples is the number of samples assumed to be normal (i.e., without anomalies)\n",
    "    pearson_threshold is the minimum value of the Pearson product-moment coefficient to have a periodic series\n",
    "    ratio_threshold is the minimum value of RStd to have an anomaly\n",
    "    '''\n",
    "    anomalies = []\n",
    "    \n",
    "    time_series = df_ts.values\n",
    "    \n",
    "    p_samples = np.round(period)\n",
    "    \n",
    "    number_of_periods = int(len(time_series)/p_samples)\n",
    "\n",
    "    for period_no in range(0, number_of_periods-1):\n",
    "        int_period = int(p_samples)\n",
    "        start_p = int(period_no*p_samples)\n",
    "        end_first_p = start_p + int_period\n",
    "        end_second_p = start_p + int_period*2\n",
    "    \n",
    "        Y = time_series[int(start_p):int(end_first_p)]\n",
    "        Z = time_series[int(end_first_p):int(end_second_p)]\n",
    "\n",
    "        pearson, _ = scipy.stats.pearsonr(Y, Z)\n",
    "\n",
    "\n",
    "        if np.abs(pearson) > pearson_threshold:\n",
    "            print(\"The series is not periodic in samples interval [%d, %d]\" %(start_p, end_second_p))\n",
    "            anomalies.append([int(start_p), int(end_second_p)])\n",
    "\n",
    "        std_current = np.std(Z)\n",
    "        std_previous = np.std(Y)\n",
    "\n",
    "        Rstd = abs(std_current-std_previous)/std_previous\n",
    "\n",
    "        if Rstd > ratio_threshold:\n",
    "            anomalies.append([int(end_first_p), int(end_second_p)])\n",
    "            print(\"Anomaly in samples interval [%d, %d]\" %(end_first_p, end_second_p))\n",
    "        \n",
    "    return anomalies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_to_onehot(intervals, length):\n",
    "    arr = np.array([0]*length)\n",
    "    for interval in intervals:\n",
    "        arr[int(interval[0]):int(interval[1]+1)] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(GT, predicted_anomalies):\n",
    "    # GT is an array of 0s and 1s\n",
    "    # predicted_anomalies is an array of 0s and 1s\n",
    "    \n",
    "    assert len(GT) == len(predicted_anomalies)\n",
    "    tp = np.sum(GT & predicted_anomalies)\n",
    "    fp = np.sum(np.invert(GT) & predicted_anomalies)\n",
    "    fn = np.sum(GT & np.invert(predicted_anomalies))\n",
    "    \n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "\n",
    "    f1 = 2/(1/precision + 1/recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1 = 0\n",
    "best_rstd = None\n",
    "\n",
    "for rstd in rstds:\n",
    "    print(\"RSTD\", rstd)\n",
    "    # generate anomalies on validation\n",
    "    anomalies = get_anomalies(val_set[\"device_consumption\"], period, rstd)\n",
    "    \n",
    "    gt = interval_to_onehot(anomalies_time_intervals, len(val_ctimes))\n",
    "    pred = interval_to_onehot(anomalies, len(val_ctimes))\n",
    "        \n",
    "    f1 = get_F1(gt, pred)\n",
    "   \n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        best_rstd = rstd\n",
    "        \n",
    "print(\"MAX F1\", max_f1, \"BEST RSTD\", best_rstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = get_anomalies(test_set[\"device_consumption\"], period, best_rstd)\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce an output file\n",
    "confidences = interval_to_onehot(anomalies, len(test_set))\n",
    "d = {'ctime': test_set[\"ctime\"], 'confidence': confidences}\n",
    "out_df = pd.DataFrame(d)\n",
    "out_df.set_index(\"ctime\", inplace=True)\n",
    "out_df.to_csv(\"basic_statistics_\" + dataset_name + \"_\" + train_length + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Anomaly detection for Univariate Time Series with Statistics and Deep Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
